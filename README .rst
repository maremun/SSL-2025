#########################
Self-Supervised Learning (SSL-2025)
#########################

Объявления
==========
1. Курс стартует осенью 2025 года.  
2. Телеграм-группа курса: `тык <https://t.me/+hV5I68RF9vA4OTgy>`_  
3. Формат: лекции + практические задания + проект.  
  
Полезные ссылки
===============
- Телеграм-группа курса: `тык <https://t.me/+hV5I68RF9vA4OTgy>`_ 
- `Материалы по PyTorch <https://pytorch.org/tutorials>`_
- Tg для связи: `TA <https://t.me/helenlyko>`_

Аннотация
=========
Self-supervised learning (SSL) — это широко используемый подход в машинном обучении, который требует минимальных знаний о домене данных и использует данные без дорогостоящей разметки для обучения полезных представлений. Полученные представления затем легко адаптируются для различных конечных задач, что позволяет создавать универсальные базовые модели.

В ходе курса вы получите прочные знания о современных методах предобучения нейронных сетей, узнаете основные идеи и концепции, теорию, лежащую в их основе, и их связь с классическими алгоритмами снижения размерности. Курс охватит ключевые аспекты контрастивных и генеративных подходов в предобучении. Вы реализуете и обучите некоторые из самых актуальных методов, а также примените их к конечным задачам, таким как классификация, сегментация, обнаружение аномалий и трансферное обучение.  

Мы также обсудим открытые исследовательские вопросы, такие как *dimensional collapse* и *task-agnostic* оценка качества представлений.  

Хотя основное внимание будет уделено работе с изображениями, мы также коснемся таких доменов, как графы и тексты, с использованием графовых нейронных сетей и трансформеров. Кроме того, мы изучим практические применения в области медицинских снимков (КТ, рентген): предобучение, сегментация, классификация патологий и обнаружение аномалий.

Курс включает **2 практических задания** и **проект**.

Цели освоения курса
===================
- Понимать основные принципы self-supervised learning (SSL).  
- Знать и уметь реализовывать современные SSL-методы.  
- Понимать теоретическую связь SSL с классическими методами обучения представлений.  
- Применять SSL-методы к реальным задачам.  
- Оценивать SSL-модели на конечных задачах.  
- Критически анализировать и сравнивать методы SSL.  
- Разрабатывать собственные SSL-решения для новых приложений.  

Пререквизиты курса 
==================
- Знание основ линейной алгебры, математического анализа и теории вероятностей.  
- Основы машинного и глубокого обучения.  
- Базовые навыки программирования на Python и владение PyTorch.  

Технические требования
======================
- Доступ к Google Colab.  
- Рекомендуется наличие GPU.  

Формула оценки и пересдачи
==========================
**Итог = Округление(0.5 * ДЗ + 0.5 * П)**  

где

- **ДЗ** — кумулятивная оценка за все домашние задания

    `ДЗ = 0.3 * ДЗ1 + 0.7 * ДЗ2`.

- **П** — оценка за проект

    `П = 0.5 * ПО + 0.5 * ПИ`

  где **ПО** — общая оценка группы, **ПИ** — индивидуальная оценка.  

Округление арифметическое.  

**Пересдача:**

- Домашние задания можно сдать со *штрафным коэффициентом 0.8* если **опоздание < 3 дней**. Далее - *коэффициент 0.6* до конца курса. 

- Проект можно пересдать, изменив только индивидуальную часть (**ПИ**) — необходимо реализовать и проанализировать дополнительный метод из современной литературы в аналогичной постановке проекта.

Курсовое домашнее задание:
===============

**Первое задание:**
************************************************
- Начало: 11.10.2025.  
- Дедлайн: 23:59 27.10.2025.
- Задание `тык <https://t.me/+hV5I68RF9vA4OTgy>`_ 
- Решение (код + отчет) прислать на почту <ssl24hse@gmail.com> с темой письма ``{ФамилияИО}-ДЗ1``, например ``ЛыковаЕА-ДЗ1``.
- Цель: реализовать одну из вспомогательных задач, обучить ResNet18 на наборе изображений.  
- Требуется:
    - Реализовать pipeline данных с аугментациями.  
    - Обучить модель на неразмеченных данных.  
    - Визуализировать полученные эмбеддинги.  
    - Провести анализ гиперпараметров.

Проект
===============
- Работа в группах 2–3 человека.  
- Цель: выбрать домен (изображения, графы, тексты) и реализовать современный SSL-метод.  
- Итог: постер + код на GitHub + презентация.  

План занятий
===============

`Вводная лекция <https://t.me/+hV5I68RF9vA4OTgy>`_
************************************************
- Проект: требования, постановка и оценивание. 
- Введение в self-supervised learning.
- SSL train pipeline.
- Linear probing.
- Плюсы SSL.
- Representation learning.
- Какие репрезентации 'хорошие'?

`Лекция 2 <https://t.me/+hV5I68RF9vA4OTgy>`_
************************************************
- Как оценивать качество получаемых репрезентаций?
- Linear probing & kNN
- *Early SSL methods*.
- Visual Pretext tasks.
- Temporal signal.
- Pretext tasks for video: Temporal Order Verification, Learning to see by moving(pred. camera transformation), Visual tracking.
- Pretext tasks for images: Relative Position, Jigsaw Puzzle, Texture and Color Ignored, Image Rotation, Discriminating among Patches, Inpainting, Colorization, Multi-tasking.
- Deep Cluster.
- Risk decomposition for SSL models, Representation usability.

`Лекция 3 <https://t.me/+hV5I68RF9vA4OTgy>`_
************************************************
- Mutual Information
- Multi-View Redundancy
- Sample Contrastive Learning: Contrastive Predictive Coding, SimCLR, MoCo.
- InfoNCE - a lower bound for MI.
- Self-Distillation: BYOL, SimSiam.

`Лекция 4 <https://t.me/+hV5I68RF9vA4OTgy>`_
************************************************
- Self-Distillation: BYOL, SimSiam.
- Clustering: SwAV.
- Collapse in Contrastive learning.
- Dimensional collapse.
- Barlow Twins.
- Information Bottleneck Principle.
- VICReg
- Decorrelation and Whitening (Soft, Cholesky, ZCA).
- W-MSE
- Decorrelated Batch-Normalization, Batch-Normalization, Shuffled Batch Normalization 
  
Рекомендуемая литература
========================
1. Bengio, Y., Courville, A., & Vincent, P. (2013). *Representation learning: A review and new perspectives*. IEEE TPAMI, 35(8), 1798-1828.  
2. Shwartz-Ziv, R., & LeCun, Y. (2023). *To Compress or Not to Compress – Self-Supervised Learning and Information Theory: A Review.* arXiv:2304.09355.  
3. Balestriero, R., & LeCun, Y. (2022). *Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods.* NeurIPS 35, 26671–26685.  
4. Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., & Goldstein, T. (2023). *A cookbook of self-supervised learning.* arXiv:2304.12210.  


Автор(ы)
=========
- Преподаватель: Мунхоева Марина Леонидовна  
- Ассистенты:  Лыкова Елена Анатольевна
